{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11914ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, datetime, sys, os\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', type=float, default = 1e-4)\n",
    "parser.add_argument('--bs', type=int, default = 2)\n",
    "parser.add_argument('--acc_grad', type=int, default = 4)\n",
    "parser.add_argument('--fold', type=int, default = 0)\n",
    "parser.add_argument('--kfolds', type=int, default = 5)\n",
    "parser.add_argument('--drop_last_layers', type=int, default = None)\n",
    "parser.add_argument('--freeze_layers', type=int, default = None)\n",
    "parser.add_argument('--freeze_embeddings', type=bool, default = False)\n",
    "parser.add_argument('--enc_depth', type=int, default = 6)\n",
    "parser.add_argument('--dec_depth', type=int, default = 6)\n",
    "parser.add_argument('--dim', type=int, default = 768)\n",
    "parser.add_argument('--aug_pct', type=float, default = 0.5)\n",
    "parser.add_argument('--aug_dir', type=str, default = \"tokens\")\n",
    "parser.add_argument('--data_dir', type=str, default = \"data\")\n",
    "parser.add_argument('--filter', action='store_true')\n",
    "parser.add_argument('--singlefold', action='store_true') #run 1 fold only (for quick testing)\n",
    "parser.add_argument('--precision', type=int, default = 16)\n",
    "parser.add_argument('--epochs', type=int, default = 20)\n",
    "parser.add_argument('--max_tokens', type=int, default = 224)\n",
    "parser.add_argument('--max_cells', type=int, default = 128)\n",
    "parser.add_argument('--graph', type=bool, default = False)\n",
    "parser.add_argument('--device',type=int, default = 0)\n",
    "parser.add_argument('--workers', type=int, default = 4) # lower to 2 or 1 (or use --use_cache) if OOM (CPU)\n",
    "parser.add_argument('--subset', type=int, default = None) #-1 all dataset, otherwise the number of samples\n",
    "parser.add_argument('--test', type=str, default = None) # path to the .csv file for running inference\n",
    "parser.add_argument('--load', type=str, default = None) \n",
    "parser.add_argument('--resume', action='store_true')\n",
    "parser.add_argument('--verbose', action='store_true')\n",
    "parser.add_argument('--experiment', type=str, default = datetime.datetime.now().strftime(\"%Y%m%d%H%M\") )\n",
    "parser.add_argument('--results_dir', type=str, default = 'results')\n",
    "parser.add_argument('--use_cache', action='store_true') # cache dataset to disk\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "args= parser.parse_args()\n",
    "print(args)\n",
    "original_cmd_line = list(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "cmdline = ['--results_dir', 'pruebas', '--singlefold','--kfolds','1000']\n",
    "sys.argv = ['_'] + cmdline\n",
    "args= parser.parse_args(cmdline)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7690bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94328480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from x_transformers import Decoder, Encoder\n",
    "import fastcore.parallel\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from functools import partial\n",
    "from methodtools import lru_cache\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar, ProgressBar\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42, workers = True)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(f\"Number of workers: {args.workers}\")\n",
    "print(f\"Number of gpus: {n_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0466002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d946548",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = Path(args.data_dir)\n",
    "p_dest_folder = Path(args.results_dir) / f'{args.experiment}'\n",
    "p_models_folder = p_dest_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CELLS  = args.max_cells\n",
    "MAX_TOKENS = args.max_tokens\n",
    "pad_token_id = 1 # CHANGE if no microsoft/codebert-base\n",
    "codebert_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = p_data / 'train'\n",
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )\n",
    "def read_notebook_and_empty(path):\n",
    "    with open(path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    d =  pd.DataFrame(data).assign(id=path.stem).rename_axis('cell_id')\n",
    "    d['source']=\"\"\n",
    "    d['cell_type']=d['cell_type'].astype('category')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c041d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_notebooks = Path('cache/notebooks-empty.feather')\n",
    "if p_notebooks.exists():\n",
    "    df = pd.read_feather(p_notebooks)\n",
    "else:\n",
    "    iterable = list(train_path.glob(\"*.json\"))\n",
    "    notebooks = fastcore.parallel.parallel(\n",
    "        read_notebook_and_empty, iterable,n_workers=63,chunksize=len(iterable)//63,progress=True)\n",
    "    df = pd.concat(notebooks).reset_index()\n",
    "    df.to_feather(p_notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index(['id','cell_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a83142",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cells_by_notebook = df['cell_type'].groupby(by='id').count()\n",
    "code_cells_by_notebook = df[df['cell_type']=='code']['cell_type'].groupby(by='id').count()\n",
    "md_cells_by_notebook=total_cells_by_notebook-code_cells_by_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.kfolds=1000 #use as much training data as possible without changing code for final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ancestors = pd.read_csv(p_data /'train_ancestors.csv',  index_col='id')\n",
    "splitter = GroupShuffleSplit(n_splits=args.kfolds, random_state=0,test_size=1/args.kfolds)\n",
    "ids = df_ancestors.index.unique('id')\n",
    "ancestors = df_ancestors.loc[ids, 'ancestor_id']\n",
    "for _ in range(1+args.fold):\n",
    "    ids_train, ids_valid = next(splitter.split(ids, groups=ancestors))\n",
    "ids_train, ids_valid = ids[ids_train], ids[ids_valid]\n",
    "print(f\"Valid items: {len(ids_valid)} {len(ids_valid)/(len(ids_train)+len(ids_valid))*100:0.3f}% of total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter notebooks MAX_CELLS\n",
    "if args.filter:\n",
    "    filter_cells = lambda xids:xids[(code_cells_by_notebook[xids.values]<=MAX_CELLS).combine(\n",
    "        md_cells_by_notebook[xids.values]<=MAX_CELLS,lambda x,y:x and y)]\n",
    "    ids_train=filter_cells(ids_train)\n",
    "    ids_valid=filter_cells(ids_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_order(path):\n",
    "    return pd.read_csv(path,index_col='id',).squeeze(\"columns\").str.split()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2227b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_aug = Path(args.aug_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tokens = Path(f'microsoft_graphcodebert-base_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec53f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_aug = [read_df_order(p) for p in sorted(p_aug.glob(\"train_orders*.csv\"))] if args.aug_pct else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21480cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_notebooks_aug = [Path(f'cache/notebooks{i}.feather') for i in range(len(df_orders_aug))] if args.aug_pct else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache (locally)\n",
    "df_aug = []\n",
    "for i,_p_notebooks_aug in tqdm(enumerate(p_notebooks_aug)):\n",
    "    if not _p_notebooks_aug.exists():\n",
    "        print(f\"Caching {_p_notebooks_aug}\")\n",
    "        tp = p_data / f'mut{i}' / 'train'\n",
    "        iterable = list(tp.glob(\"*.json\"))\n",
    "        notebooks_aug = fastcore.parallel.parallel(\n",
    "            read_notebook_and_empty, iterable,n_workers=63,chunksize=len(iterable)//63,progress=True)\n",
    "        _df_aug = pd.concat(notebooks_aug).reset_index()\n",
    "        _df_aug.to_feather(_p_notebooks_aug)\n",
    "    else:\n",
    "        print(f\"Reading {_p_notebooks_aug}\")\n",
    "        _df_aug = pd.read_feather(_p_notebooks_aug)\n",
    "    _df_aug=_df_aug.set_index(['id','cell_id'])\n",
    "    df_aug.append(_df_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tokens_aug = sorted(p_aug.glob(\"microsoft_graphcodebert-base_tokens*\")) if args.aug_pct else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d692cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.aug_pct:\n",
    "    assert len(p_tokens_aug) == len(df_orders_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79742993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.test is None:\n",
    "    df_orders = read_df_order(p_data / 'train_orders.csv')\n",
    "else: \n",
    "    df_orders = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(_df_orders):\n",
    "    return _df_orders[_df_orders.index.isin(np.concatenate((ids_train.values,ids_valid.values)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders=filter_df(df_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7766c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.aug_pct:\n",
    "    df_orders_aug = [filter_df(_d) for _d in df_orders_aug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541d710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#noexport\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_ints = np.vectorize(partial(int,base=16))\n",
    "p_tokens = Path(f'microsoft_graphcodebert-base_tokens')\n",
    "\n",
    "def get_ids(notebook_id,df):\n",
    "    data = df.loc[notebook_id]       \n",
    "    return data.index[data['cell_type']=='code'].values,data.index[data['cell_type']=='markdown'].values\n",
    "\n",
    "def get_index_gt(gt, cc, md):\n",
    "    xc= torch.full((MAX_CELLS,), -1)\n",
    "    xm= torch.full((MAX_CELLS,), -1)\n",
    "    d= {v:i for i, v in enumerate(gt)}\n",
    "    c= torch.tensor([d[c] for c in cc])\n",
    "    m= torch.tensor([d[m] for m in md])\n",
    "        \n",
    "    xc[:min(MAX_CELLS, len(c))]=c[:min(MAX_CELLS, len(c))]\n",
    "    xm[:min(MAX_CELLS, len(m))]=m[:min(MAX_CELLS, len(m))]\n",
    "        \n",
    "    return torch.stack((xc,  xm))\n",
    "\n",
    "class TokensDataset(Dataset):\n",
    "    def __init__(self,idx,aug_pct):\n",
    "        self.df_orders = df_orders.loc[idx]\n",
    "        self.df_orders_aug = [_d.loc[idx] for _d in df_orders_aug]\n",
    "        self.aug_pct = aug_pct\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_orders)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.rand((1,)).item() < self.aug_pct:\n",
    "            # augment\n",
    "            aug_idx = torch.randint(len(self.df_orders_aug),(1,)).item()\n",
    "            df_orders = self.df_orders_aug[aug_idx]\n",
    "            pp_tokens = p_tokens_aug[aug_idx]\n",
    "            dff = df_aug[aug_idx]\n",
    "        else:\n",
    "            # not augment\n",
    "            df_orders = self.df_orders\n",
    "            pp_tokens = p_tokens\n",
    "            dff = df\n",
    "\n",
    "        notebook_id = df_orders.index[idx]\n",
    "    \n",
    "        code_tokens = torch.full((MAX_CELLS,MAX_TOKENS),pad_token_id,dtype=torch.int64)\n",
    "        md_tokens   = torch.full((MAX_CELLS,MAX_TOKENS),pad_token_id,dtype=torch.int64)\n",
    "        \n",
    "        _code_tokens = torch.from_numpy(np.load(str(pp_tokens / notebook_id)+\"_code.npy\"))[:,:MAX_TOKENS]\n",
    "        \n",
    "        n_c = len(_code_tokens)\n",
    "        code_pos = torch.arange(n_c) if n_c <= MAX_CELLS else torch.concat((torch.IntTensor([0]),\n",
    "                                 (torch.randperm(n_c-2)+1)[:MAX_CELLS-2],\n",
    "                                 torch.IntTensor([n_c-1])))\n",
    "        \n",
    "        #to make sure no cheating\n",
    "        code_pos = code_pos[torch.randperm(len(code_pos))]\n",
    "\n",
    "        _code_tokens = _code_tokens[code_pos]\n",
    "        \n",
    "        code_pos_pct = torch.full((MAX_CELLS,),-1.)\n",
    "        code_pos_pct[:n_c] = (code_pos.float()/(n_c-1)).nan_to_num(posinf=0) # set n_c = 1 with 0.\n",
    "        \n",
    "        # TODO: Deal with _md_tokens > MAX_CELLS in inference\n",
    "        _md_tokens = torch.from_numpy(np.load(str(pp_tokens / notebook_id)+\"_md.npy\"))[:,:MAX_TOKENS]\n",
    "        n_md = len(_md_tokens)\n",
    "        md_pos = torch.randperm(n_md)[:MAX_CELLS]\n",
    "        _md_tokens = _md_tokens[md_pos]\n",
    "        \n",
    "        \n",
    "        # filter ids by sampled positions\n",
    "        gt_ids = df_orders[notebook_id]\n",
    "        code_ids, md_ids = get_ids(notebook_id,dff)\n",
    "        code_ids = code_ids[code_pos.tolist()]\n",
    "        md_ids   = md_ids[md_pos.tolist()]\n",
    "        gt_ids = [i for i in gt_ids if (i in code_ids) or (i in md_ids)]\n",
    "        index_gt = get_index_gt(gt_ids, code_ids, md_ids)\n",
    "        \n",
    "        code_tokens[:len(_code_tokens),:_code_tokens.shape[1]] = _code_tokens\n",
    "        md_tokens  [:len(_md_tokens),:_md_tokens.shape[1]]     = _md_tokens\n",
    "            \n",
    "        return code_pos_pct, code_tokens, md_tokens, index_gt, notebook_id\n",
    "    \n",
    "train_ds,valid_ds = TokensDataset(ids_train,args.aug_pct),TokensDataset(ids_valid,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0465f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "train_dl = DataLoader(train_ds,args.bs,   shuffle=True,  num_workers=args.workers, pin_memory=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds,args.bs*2, shuffle=False, num_workers=args.workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42800201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "code_pos_pct, code_tokens, md_tokens, index_gt, notebook_ids = next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_bert=RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pad_token_id == code_bert.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self,*args):\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6452baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.drop_last_layers:\n",
    "    for l in range(len(code_bert.encoder.layer)-args.drop_last_layers,len(code_bert.encoder.layer)):\n",
    "        code_bert.encoder.layer[l]=Identity()\n",
    "        \n",
    "modules = []\n",
    "if args.freeze_layers:\n",
    "    modules.append(code_bert.encoder.layer[:args.freeze_layers])\n",
    "if args.freeze_embeddings:\n",
    "    modules.append(code_bert.embeddings.word_embeddings)\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad =False\n",
    "code_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e68881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def positionalencoding1d(positions_pct, d_model):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the model\n",
    "    :param length: length of positions\n",
    "    :return: length*d_model position matrix\n",
    "    \"\"\"\n",
    "    device,dtype = positions_pct.device, positions_pct.dtype\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dim (got dim={:d})\".format(d_model))\n",
    "    pe = torch.empty((*positions_pct.shape[:2], d_model),device=device,dtype=dtype)\n",
    "    position = (positions_pct * 50.).flatten(0).unsqueeze(1)\n",
    "    div_term = torch.exp((torch.arange(0, d_model, 2, device=device,dtype=dtype) *\n",
    "                         -(math.log(10000.0) / d_model)))\n",
    "    pe[..., 0::2] = torch.sin(position * div_term).view(*positions_pct.shape[:2],-1)\n",
    "    pe[..., 1::2] = torch.cos(position * div_term).view(*positions_pct.shape[:2],-1)\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e171d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orderer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "                \n",
    "        encoder_dim = args.dim \n",
    "        \n",
    "        self.codebert_proj = nn.Linear(codebert_dim,encoder_dim,bias=False) \\\n",
    "            if codebert_dim!=encoder_dim else nn.Identity()\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            dim = encoder_dim,\n",
    "            depth = args.enc_depth,\n",
    "            heads = 8,\n",
    "            attn_num_mem_kv = 16,\n",
    "            rotary_pos_emb = False,\n",
    "        )\n",
    "        \n",
    "        self.decoder = Encoder(\n",
    "            dim = encoder_dim,\n",
    "            depth = args.dec_depth,\n",
    "            heads = 8,\n",
    "            attn_num_mem_kv = 16,\n",
    "            rotary_pos_emb = False,\n",
    "            cross_attend = True,\n",
    "        )\n",
    "\n",
    "       \n",
    "        self.md_regressor   = nn.Sequential(nn.Linear(encoder_dim, 2, bias = True), nn.GLU())\n",
    "        \n",
    "        self.code_bert = code_bert\n",
    "        \n",
    "    def forward(self, code_pos_pct, code_tokens, md_tokens):\n",
    "        bs = code_tokens.shape[0]\n",
    "        device = code_tokens.device\n",
    "        \n",
    "        code_mask = code_tokens != pad_token_id\n",
    "        x_code = self.code_bert(code_tokens.flatten(0,1),\n",
    "                                attention_mask=code_mask.flatten(0,1).to(device))\\\n",
    "                .last_hidden_state.view(*code_tokens.shape[:3],-1)[:,:,0,:]\n",
    "        x_code = self.codebert_proj(x_code)\n",
    "        \n",
    "        md_mask = md_tokens != pad_token_id\n",
    "        x_md = self.code_bert(md_tokens.flatten(0,1),\n",
    "                                attention_mask=md_mask.flatten(0,1).to(device))\\\n",
    "                .last_hidden_state.view(*md_tokens.shape[:3],-1)[:,:,0,:]\n",
    "        x_md = self.codebert_proj(x_md)\n",
    "        \n",
    "        m_code = ~(code_tokens == pad_token_id).all(dim=-1)\n",
    "        m_md   = ~(md_tokens == pad_token_id).all(dim=-1)\n",
    "                \n",
    "        nc = m_code.sum(dim=1)\n",
    "        nm = m_md.sum(dim=1)\n",
    "        \n",
    "                \n",
    "        x_code += positionalencoding1d(code_pos_pct,x_code.shape[-1])\n",
    "        \n",
    "        x_code = self.encoder(x_code, mask = m_code)\n",
    "        x_md   = self.decoder(x_md, context = x_code, mask = m_md, context_mask = m_code)\n",
    "                \n",
    "        encoded_all = (nc+nm).view(bs,1,1) * torch.cat((\n",
    "            code_pos_pct.unsqueeze(1),\n",
    "            -0.2 + 1.4 * self.md_regressor(x_md).sigmoid().view(bs,1,-1)\n",
    "        ),1)\n",
    "                \n",
    "        return encoded_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_to_ids_tensor(preds,masks,notebook_ids):\n",
    "    device = preds.device\n",
    "    preds=preds.clone()\n",
    "    preds[~masks]=np.inf\n",
    "    orders=preds.detach().flatten(1).argsort().argsort().view(*preds.shape).cpu().numpy()\n",
    "    print(f\"preds_to_ids_tensor {orders}\")\n",
    "    predictions,ground_truth=[],[]\n",
    "    for notebook_id,order,mask in zip(notebook_ids,orders,masks):\n",
    "        code_ids,md_ids=get_ids(notebook_id)\n",
    "        nc,nm=len(code_ids),len(md_ids)\n",
    "        gt = df_orders[notebook_id] if notebook_id in df_orders else None\n",
    "        assert (nc+nm) == len(gt)\n",
    "        code_order,md_order=order[0],order[1]\n",
    "        pred=torch.full((len(gt),),-1,dtype=torch.int64,device=device)\n",
    "        pred[code_order[:nc]] =  torch.from_numpy(ids_to_ints(code_ids)).to(device)\n",
    "        pred[md_order[:nm]] =  torch.from_numpy(ids_to_ints(md_ids)).to(device)\n",
    "        predictions.append(pred)\n",
    "        if gt: ground_truth.append(torch.from_numpy(ids_to_ints(gt)).to(device))\n",
    "    return predictions,ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "m=Orderer()\n",
    "preds = m(code_pos_pct, code_tokens, md_tokens)\n",
    "print(preds[0],md_tokens[0],index_gt[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53372644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsort import soft_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd00e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        print(ranks)\n",
    "        total_inversions += count_inversions_slowly(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n",
    "\n",
    "count_inversions_tensor = lambda r:(r.unsqueeze(0).t()>r).triu().sum()\n",
    "\n",
    "def count_inversions_slowly(ranks):\n",
    "    inversions = 0\n",
    "    size = len(ranks)\n",
    "    for i in range(size):\n",
    "        for j in range(i+1, size):\n",
    "            if ranks[i] > ranks[j]:\n",
    "                inversions += 1\n",
    "                \n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau_tensor(ground_truth, predictions):\n",
    "    #print(len(ground_truth), len(predictions))\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = torch.nonzero(pred[..., None] == gt)[:,1]\n",
    "        total_inversions += count_inversions_tensor(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n",
    "\n",
    "def soft_kendall_tau_tensor(ground_truth, predictions):\n",
    "    #print(len(ground_truth), len(predictions))\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        rank_gt = gt.argsort().argsort()\n",
    "        ranks = soft_rank(pred.unsqueeze(0),regularization_strength=1.)[0][rank_gt]-1\n",
    "        total_inversions += (ranks.unsqueeze(0).t()-ranks).triu().sum()\n",
    "        print(total_inversions)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 4 * total_inversions / total_2max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175278f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "class KendallTau(torchmetrics.Metric):\n",
    "    # Set to True if the metric during 'update' requires access to the global metric\n",
    "    # state for its calculations. If not, setting this to False indicates that all\n",
    "    # batch states are independent and we will optimize the runtime of 'forward'\n",
    "    is_differentiable: bool = False\n",
    "    higher_is_better: bool = True\n",
    "    full_state_update: bool = False\n",
    "    inversions: torch.LongTensor\n",
    "    total_2max: torch.LongTensor\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"inversions\", default=torch.LongTensor([0]), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total_2max\", default=torch.LongTensor([0]), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update_slowly(self, preds: list, target: list):\n",
    "        for gt, pred in zip(target, preds):\n",
    "            ranks = torch.nonzero(pred[..., None] == gt)[:,1]\n",
    "            #print(f\"update: {ranks}\")\n",
    "            self.inversions += count_inversions_slowly(ranks)\n",
    "            n = len(gt)\n",
    "            self.total_2max += n * (n-1)\n",
    "                    \n",
    "    def update(self,preds,index_gt):\n",
    "        masks = index_gt != -1\n",
    "        _preds=preds.clone()\n",
    "        _index_gt = index_gt.clone()\n",
    "        m = masks.flatten(1)\n",
    "        _index_gt[~masks] = MAX_CELLS*2\n",
    "        _preds[~masks] = np.inf\n",
    "        pred_ranks = _preds.flatten(1).argsort(dim=1)\n",
    "        ranks=torch.gather(_index_gt.flatten(1),1,pred_ranks)\n",
    "        self.inversions += torch.triu((ranks.unsqueeze(1).permute(0,2,1)>ranks.unsqueeze(1))).sum()\n",
    "        len_gt = masks.sum(dim=(1,2))\n",
    "        self.total_2max +=((len_gt-1)*len_gt).sum()\n",
    "\n",
    "    def compute(self):\n",
    "        return  1. - 4. * self.inversions /  self.total_2max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,args.bs, shuffle=True,  num_workers=args.workers, pin_memory=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds,args.bs, shuffle=False, num_workers=args.workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd68c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "example = next(iter(train_dl))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_to_ids(preds,masks,notebook_ids):\n",
    "    preds[~masks]=np.inf\n",
    "    orders=preds.detach().flatten(1).argsort().argsort().view(*preds.shape).cpu().numpy()\n",
    "    predictions,ground_truth=[],[]\n",
    "    for notebook_id,order,mask in zip(notebook_ids,orders,masks):\n",
    "        code_ids,md_ids=get_ids(notebook_id)\n",
    "        nc,nm=len(code_ids),len(md_ids)\n",
    "        gt = df_orders[notebook_id] if notebook_id in df_orders else None\n",
    "        assert (nc+nm) == len(gt)\n",
    "        code_order,md_order=order[0],order[1]\n",
    "        pred=np.full((len(gt)),None,dtype=object)\n",
    "        pred[code_order[:nc]] =  code_ids\n",
    "        pred[md_order[:nm]] =  md_ids\n",
    "        predictions.append(pred.tolist())\n",
    "        if gt: ground_truth.append(gt)\n",
    "    return predictions,ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "code_pos_pct, code_tokens, md_tokens,index_gt,notebook_ids = example\n",
    "m=Orderer()\n",
    "preds = m(code_pos_pct, code_tokens, md_tokens)\n",
    "print(preds,index_gt[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "k=KendallTau()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nannorm(t,**kw): return (t*t).nansum(**kw).sqrt()\n",
    "\n",
    "def spearman_rho(pred, target, **kw):\n",
    "    pred = soft_rank(pred, **kw)\n",
    "    target = soft_rank(target, **kw)\n",
    "    pred = pred - pred.mean()\n",
    "    pred = pred / pred.norm()\n",
    "    target = target - target.mean()\n",
    "    target = target / target.norm()\n",
    "    return (pred * target).sum()\n",
    "\n",
    "def spearman_rho_loss(pred, target, **kw):\n",
    "    loss = 0.\n",
    "    for p,t in zip(pred,target):\n",
    "        mask = t == -1\n",
    "        loss += spearmanr_(p[~mask].unsqueeze(0), t[~mask].unsqueeze(0), **kw)\n",
    "    return 1. - loss/pred.shape[0]\n",
    "\n",
    "max_non_inversions = \\\n",
    "    [(torch.arange(n).unsqueeze(0).t()-torch.arange(n)).triu().sum().item() for n in range(2*MAX_CELLS)]\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Rank_correlation#Spearman%E2%80%99s_%CF%81_as_a_particular_case\n",
    "def spearman_p_loss(pred, target_ranks, **kw):\n",
    "    d2 = 0.\n",
    "    n = 0.\n",
    "    for p,t in zip(pred.flatten(1),target_ranks.flatten(1)):\n",
    "        mask = t != -1\n",
    "        rp = soft_rank(p[mask].unsqueeze(0), **kw) - 1.\n",
    "        rt = t[mask].unsqueeze(0)\n",
    "        d2 += (rt-rp).square().sum()\n",
    "        n  += mask.sum()\n",
    "    return 4. * d2/ ((n*(n-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ed7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdererModule(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 lr: float = 2e-5,\n",
    "                 adam_epsilon: float = 1e-8,\n",
    "                 weight_decay: float = 0.01,\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Create model\n",
    "        self.model = Orderer()\n",
    "        # Create loss module\n",
    "        self.loss_module = spearman_p_loss\n",
    "        \n",
    "        #self.mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.kendall_tau = KendallTau()\n",
    "        \n",
    "        self.batch_size = args.bs\n",
    "        self.automatic_optimization = True        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                           if ('code_bert' in n) and not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "                \"lr\" : self.hparams.lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                           if ('code_bert' in n) and  any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\" : self.hparams.lr,\n",
    "\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                           if ('code_bert' not in n) and not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "                \"lr\" : 1. * self.hparams.lr,\n",
    "\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() \n",
    "                           if ('code_bert' not in n) and  any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\" : 1. * self.hparams.lr,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.lr, eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0.05 * self.trainer.estimated_stepping_batches,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        \n",
    "        print(optimizer,scheduler)\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]    \n",
    "    \n",
    "    def forward(self, *imgs):\n",
    "        # Forward function that is run when visualizing the graph\n",
    "        #print(imgs.shape)\n",
    "        return self.model(*imgs)    \n",
    "           \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        code_pos_pct, code_tokens, md_tokens,index_gt, notebook_ids = batch\n",
    "        \n",
    "        \n",
    "        preds = self.model(code_pos_pct, code_tokens, md_tokens)\n",
    "        loss = self.loss_module(preds, index_gt)\n",
    "        \n",
    "        self.log('train_loss', loss.detach(), on_step=True, on_epoch=True, sync_dist=True)\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        code_pos_pct, code_tokens, md_tokens, index_gt, notebook_ids = batch\n",
    "        \n",
    "        preds = self.model(code_pos_pct, code_tokens, md_tokens)\n",
    "        loss = self.loss_module(preds, index_gt)\n",
    "        \n",
    "        self.kendall_tau.update(preds.detach(),index_gt)\n",
    "        \n",
    "        self.log('val_loss', loss.detach(), on_step=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        kendall_tau=self.kendall_tau.compute()\n",
    "        print(kendall_tau)\n",
    "        self.log('val_kendall_tau', kendall_tau,sync_dist=True,prog_bar=True)\n",
    "        self.kendall_tau.reset()\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        code_pos_pct, code_tokens, md_tokens,index_gt,notebook_ids = batch        \n",
    "        \n",
    "        preds = self.model(code_pos_pct, code_tokens, md_tokens)\n",
    "                \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        code_pos_pct, code_tokens, md_tokens,index_gt,notebook_ids = batch\n",
    "        \n",
    "        mask = labels != -1\n",
    "        preds = self.model(code_pos_pct, code_tokens, md_tokens)\n",
    "        \n",
    "        return  preds\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        sch = self.lr_schedulers()\n",
    "\n",
    "        # If the selected scheduler is a ReduceLROnPlateau scheduler.\n",
    "        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            sch.step(self.trainer.callback_metrics[\"train_loss\"])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitProgressBar(TQDMProgressBar):\n",
    "\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        bar.set_description('running validation ...')\n",
    "        return bar\n",
    "\n",
    "bar = LitProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df993f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.profiler import AdvancedProfiler\n",
    "ap = AdvancedProfiler(filename = 'advanced_profiler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.load and not args.resume:\n",
    "    module = OrdererModule.load_from_checkpoint(args.load,lr=args.lr)\n",
    "else:\n",
    "    module = OrdererModule(lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointfilename = f'{args.experiment}-{args.fold+1}of{args.kfolds}-' + \\\n",
    "    '{epoch}-{train_loss:.2f}-{val_loss:.2f}-{val_kendall_tau:.4f}'\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint( save_weights_only=False, mode=\"max\", monitor=\"val_kendall_tau\", \n",
    "                                  save_last=True,save_top_k=-1,\n",
    "                                  filename= checkpointfilename,verbose=True)\n",
    "\n",
    "modelcheckpoint.CHECKPOINT_NAME_LAST=checkpointfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76bf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(logger=None,\n",
    "    precision=args.precision,\n",
    "    accelerator = 'gpu',\n",
    "    strategy=pl.strategies.DDPStrategy(find_unused_parameters=True) if n_gpus >1 else None,\n",
    "    gpus = n_gpus,\n",
    "    resume_from_checkpoint=args.load if args.resume else None,\n",
    "    max_epochs= args.epochs, \n",
    "    deterministic = False, \n",
    "    callbacks=[modelcheckpoint,LearningRateMonitor(\"step\"),],\n",
    "    accumulate_grad_batches=args.acc_grad,\n",
    "    )             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1af5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(module, train_dataloaders=train_dl, val_dataloaders=valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51729746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
